{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "create a pytorch Dataset from MVS10015 Dataset\n",
    "pytorch [documentaion](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from skimage import io\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split (80%)/(20%):\n",
    "\n",
    "source_folder = 'Data/HAM100000_images/'\n",
    "test_folder = 'Data/HAM100000_images_test/'\n",
    "train_folder = 'Data/HAM100000_images_train/'\n",
    "\n",
    "file_list = os.listdir(source_folder)\n",
    "\n",
    "number_test_images = math.ceil(0.2 * len(file_list))\n",
    "number_train_images = len(file_list) - number_test_images\n",
    "# TODO: Set manual seed and select number_test_images in and rest in 'Data/HAM100000_images_train' \n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "random_numbers = set()\n",
    "\n",
    "while len(random_numbers) <= number_test_images:\n",
    "    random_numbers.add(random.randint(0, len(file_list)-1))\n",
    "\n",
    "for index, file in enumerate(file_list):\n",
    "    if index in random_numbers:\n",
    "        source_path = os.path.join(source_folder, file)\n",
    "        destination_path = os.path.join(train_folder, file)\n",
    "        shutil.copy(source_path, destination_path)\n",
    "    else:\n",
    "        source_path = os.path.join(source_folder, file)\n",
    "        destination_path = os.path.join(test_folder, file)\n",
    "        shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id\n"
     ]
    }
   ],
   "source": [
    "# Create annotations file for given train/test split\n",
    "\n",
    "test_directory = 'Data/HAM100000_images_test/'\n",
    "\n",
    "with open('Data/HAM10000_labels.csv') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    print(next(csv_reader)[0])\n",
    "    write_test = []\n",
    "    write_train = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        path_image = os.path.join(test_directory, row[0])\n",
    "        path_image = path_image + '.jpg'\n",
    "        if os.path.isfile(path_image):\n",
    "            write_test.append([row[0], row[1]])\n",
    "        else:\n",
    "            write_train.append([row[0], row[1]])\n",
    "    \n",
    "with open('Data/HAM100000_train_label.csv', 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for row in write_train:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "\n",
    "with open('Data/HAM100000_test_label.csv', 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    for row in write_test:\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\\HAM100000_images\\ISIC_0025030.jpg\n"
     ]
    }
   ],
   "source": [
    "# show an image\n",
    "root = 'Data\\HAM100000_images'\n",
    "frame = pd.read_csv('Data\\HAM10000_labels.csv')\n",
    "\n",
    "image_path = os.path.join(root, frame.iloc[0,0])\n",
    "image_path = image_path + '.jpg'\n",
    "with Image.open(image_path) as image:\n",
    "    image.show()\n",
    "\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[194.69792020561766, 139.26262746509832, 145.48524135685338] [22.855094582223956, 30.168411555547745, 33.903190491317204]\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and std of dataset, all images have size (600, 450), after transforming images to tensors they have size (3, 600, 450)\n",
    "file_list = os.listdir('Data\\HAM100000_images')\n",
    "mean = [0, 0, 0]\n",
    "std = [0, 0, 0]\n",
    "\n",
    "for file_name in file_list:\n",
    "    image_path = os.path.join('Data\\HAM100000_images', file_name)\n",
    "    image = io.imread(image_path)\n",
    "    mean[0] += image[:, :, 0].mean()\n",
    "    mean[1] += image[:, :, 1].mean()\n",
    "    mean[2] += image[:, :, 2].mean()\n",
    "    std[0] += image[:, :, 0].std()\n",
    "    std[1] += image[:, :, 1].std()\n",
    "    std[2] += image[:, :, 2].std()    \n",
    "for i in [0, 1, 2]:\n",
    "    mean[i] /= len(file_list)\n",
    "    std[i] /= len(file_list)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[194.69792020561766, 139.26262746509832, 145.48524135685338], std=[22.855094582223956, 30.168411555547745, 33.903190491317204])\n",
    "])\n",
    "\n",
    "# to use these data needs to be in class folders\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='Data/HAM100000_images_train/', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='Data/HAM100000_images_test/', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 450, 600])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "test = next(iter(train_loader))\n",
    "print(type(test))\n",
    "print(len(test))\n",
    "print(type(test[0]))\n",
    "print(test[0].shape)\n",
    "print(type(test[1]))\n",
    "print(test[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, dropout = False):\n",
    "        super(net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 75 * 100, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 7)\n",
    "\n",
    "        # less droupout in deeper layers\n",
    "        self.dropout = False \n",
    "        self.dropout_3 = nn.Dropout(p=0.3) \n",
    "        self.dropout_5 = nn.Dropout(p=0.5)\n",
    "    def forward(self, x, train=True):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 32 * 75 * 100)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        if train and self.dropout:\n",
    "            x = self.dropout_5(x)    \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        if train and self.dropout:\n",
    "            x = self.dropout_3(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "classes = ('akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train_model(model, data_loader, epochs, lr=0.1, optim=None):\n",
    "  if optim is None:\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "  else:\n",
    "    optimizer = optim\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  \n",
    "  model.to(device)\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      y_pred = model(X)\n",
    "      y_one_hot = torch.nn.functional.one_hot(y, num_classes=7)\n",
    "      loss = loss_fn(y_pred, y_one_hot)\n",
    "      epoch_loss += loss\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    print(f\"average batch loss in {epoch+1}: {epoch_loss/batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loop\n",
    "def test_model(model, data_loader):\n",
    "  model.to(device)\n",
    "  with torch.no_grad():\n",
    "    right, wrong = np.array([0 for i in range(10)]), np.array([0 for i in range(10)])\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      y_pred = model(X, train=False)\n",
    "      y_pred = torch.argmax(y_pred, dim=1)\n",
    "      for i in range(len(y_pred)):\n",
    "        if y_pred[i]==y[i]:\n",
    "          right[y[i]] += 1\n",
    "        else:\n",
    "         wrong[y[i]] += 1\n",
    "\n",
    "  for i in range(10):\n",
    "    print(f\"accuracy for {classes[i]}: {right[i]}/{right[i]+wrong[i]} | accuracy in %: {right[i]/(right[i]+wrong[i])*100}\")\n",
    "  \n",
    "  print(f\"overall accuracy: {np.sum(right)} / {np.sum(right) + np.sum(wrong)} | accuracy in percent % {100*np.sum(right)/(np.sum(right) + np.sum(wrong))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(train_loader))\n",
    "model_0 = net()\n",
    "bing = model_0(test[0])\n",
    "# train_model(model_0, train_loader, epochs=3, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1152, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 450, 600])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bing.shape)\n",
    "test[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
