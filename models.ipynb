{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nyywiq9ySJH7"
      },
      "outputs": [],
      "source": [
        "# dependencies\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from skimage import io\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "import skimage.io as io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connecting to drive to save models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "I3MNd1SVSSPx",
        "outputId": "5a393ddf-d5c5-4b8e-b648-89548f356f4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXtdTc7tSJH9"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNrCT8lpSJH-"
      },
      "source": [
        "### (600,450)\n",
        "creates dataloader, that holds a tensor representation of the whole (600,450) image. This is to big to work with.v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YTuw0HdESJH-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2defad6a-098c-4817-c501-eb724ec141f3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4080b8fc3dda>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Data/HAM100000_images_train/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnumber_test_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/HAM100000_images/'"
          ]
        }
      ],
      "source": [
        "# Create train/test split (80%)/(20%):\n",
        "\n",
        "source_folder = 'Data/HAM100000_images/'\n",
        "test_folder = 'Data/HAM100000_images_test/'\n",
        "train_folder = 'Data/HAM100000_images_train/'\n",
        "\n",
        "file_list = os.listdir(source_folder)\n",
        "\n",
        "number_test_images = math.ceil(0.2 * len(file_list))\n",
        "number_train_images = len(file_list) - number_test_images\n",
        "# TODO: Set manual seed and select number_test_images in and rest in 'Data/HAM100000_images_train' \n",
        "seed_value = 42\n",
        "random.seed(seed_value)\n",
        "\n",
        "random_numbers = set()\n",
        "\n",
        "while len(random_numbers) <= number_test_images:\n",
        "    random_numbers.add(random.randint(0, len(file_list)-1))\n",
        "\n",
        "for index, file in enumerate(file_list):\n",
        "    if index in random_numbers:\n",
        "        source_path = os.path.join(source_folder, file)\n",
        "        destination_path = os.path.join(train_folder, file)\n",
        "        shutil.copy(source_path, destination_path)\n",
        "    else:\n",
        "        source_path = os.path.join(source_folder, file)\n",
        "        destination_path = os.path.join(test_folder, file)\n",
        "        shutil.copy(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzTHPNBVSJH-"
      },
      "outputs": [],
      "source": [
        "# Create annotations file for given train/test split\n",
        "\n",
        "test_directory = 'Data/HAM100000_images_test/'\n",
        "\n",
        "with open('Data/HAM10000_labels.csv') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    print(next(csv_reader)[0])\n",
        "    write_test = []\n",
        "    write_train = []\n",
        "\n",
        "    for row in csv_reader:\n",
        "        path_image = os.path.join(test_directory, row[0])\n",
        "        path_image = path_image + '.jpg'\n",
        "        if os.path.isfile(path_image):\n",
        "            write_test.append([row[0], row[1]])\n",
        "        else:\n",
        "            write_train.append([row[0], row[1]])\n",
        "    \n",
        "with open('Data/HAM100000_train_label.csv', 'w', newline='') as file:\n",
        "    csv_writer = csv.writer(file)\n",
        "    for row in write_train:\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "\n",
        "with open('Data/HAM100000_test_label.csv', 'w', newline='') as file:\n",
        "    csv_writer = csv.writer(file)\n",
        "    for row in write_test:\n",
        "        csv_writer.writerow(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMYXhVKxSJH_",
        "outputId": "bb81b3c6-a6cb-42db-efb9-1836c83b2ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data\\HAM100000_images\\ISIC_0025030.jpg\n"
          ]
        }
      ],
      "source": [
        "# show an image\n",
        "root = 'Data\\HAM100000_images'\n",
        "frame = pd.read_csv('Data\\HAM10000_labels.csv')\n",
        "\n",
        "image_path = os.path.join(root, frame.iloc[0,0])\n",
        "image_path = image_path + '.jpg'\n",
        "with Image.open(image_path) as image:\n",
        "    image.show()\n",
        "\n",
        "print(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30fcXuG9SJIA",
        "outputId": "af5aab65-0d81-44e9-ebb2-543e2c6aa684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[194.69792020561766, 139.26262746509832, 145.48524135685338] [22.855094582223956, 30.168411555547745, 33.903190491317204]\n"
          ]
        }
      ],
      "source": [
        "# calculate mean and std of dataset, all images have size (600, 450), after transforming images to tensors they have size (3, 600, 450)\n",
        "file_list = os.listdir('Data\\HAM100000_images')\n",
        "mean = [0, 0, 0]\n",
        "std = [0, 0, 0]\n",
        "\n",
        "for file_name in file_list:\n",
        "    image_path = os.path.join('Data\\HAM100000_images', file_name)\n",
        "    image = io.imread(image_path)\n",
        "    mean[0] += image[:, :, 0].mean()\n",
        "    mean[1] += image[:, :, 1].mean()\n",
        "    mean[2] += image[:, :, 2].mean()\n",
        "    std[0] += image[:, :, 0].std()\n",
        "    std[1] += image[:, :, 1].std()\n",
        "    std[2] += image[:, :, 2].std()    \n",
        "for i in [0, 1, 2]:\n",
        "    mean[i] /= len(file_list)\n",
        "    std[i] /= len(file_list)\n",
        "\n",
        "print(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qjgnebYSJIA"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj8XggxdSJIA"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[194.69792020561766, 139.26262746509832, 145.48524135685338], std=[22.855094582223956, 30.168411555547745, 33.903190491317204])\n",
        "])\n",
        "\n",
        "# to use these data needs to be in class folders\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root='Data/HAM100000_images_train/', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root='Data/HAM100000_images_test/', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AVd9hfxSJIB",
        "outputId": "f08137fc-94ce-4d15-e6b8-5be4825238c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "2\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([32, 3, 450, 600])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "test = next(iter(train_loader))\n",
        "print(type(test))\n",
        "print(len(test))\n",
        "print(type(test[0]))\n",
        "print(test[0].shape)\n",
        "print(type(test[1]))\n",
        "print(test[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwuQy5dXSJIB"
      },
      "source": [
        "### (28,28)\n",
        "With the dataset there comes a csv containing the same images compressed to (28,28), in a csv file. This is better to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JUMHMDgDSJIB"
      },
      "outputs": [],
      "source": [
        "# generate train/test split\n",
        "random.seed(42)\n",
        "test_split  = set()\n",
        "while len(test_split) < 2000:\n",
        "    test_split.add(random.randint(0, 10015-1)) # HAM10000 has 10015 images\n",
        "\n",
        "#read csv file into list\n",
        "\n",
        "with open('drive/MyDrive/SkinCancer/hmnist_28_28_RGB.csv') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    next(csv_reader)\n",
        "    data = []\n",
        "    for row in csv_reader:\n",
        "        data.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nGiO7logSJIB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CSV_Data(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "xgOS6oW2SJIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9107c7e-5009-4933-e50a-e3f1ff0a585a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "501\n",
            "125\n",
            "8016\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "shuffle = True\n",
        "\n",
        "train_input = []\n",
        "train_labels = []\n",
        "test_input = []\n",
        "test_labels = []\n",
        "\n",
        "for index, image in enumerate(data):\n",
        "    image = [float(pixel) for pixel in image]\n",
        "    if index in test_split:\n",
        "        test_input.append(torch.tensor([[image[:-1][0:28*28][i*28:(i+1)*28] for i in range(28)], [image[:-1][28*28:2*28*28][i*28:(i+1)*28] for i in range(28)], [image[:-1][2*28*28:][i*28:(i+1)*28] for i in range(28)]], dtype=torch.float32)) # 3 channels\n",
        "        test_labels.append(torch.tensor(image[-1], dtype=torch.long))\n",
        "    else:\n",
        "        train_input.append(torch.tensor([[image[:-1][0:28*28][i*28:(i+1)*28] for i in range(28)], [image[:-1][28*28:2*28*28][i*28:(i+1)*28] for i in range(28)], [image[:-1][2*28*28:][i*28:(i+1)*28] for i in range(28)]], dtype=torch.float32))\n",
        "        train_labels.append(torch.tensor(image[-1], dtype=torch.long))\n",
        "train_loader = DataLoader(CSV_Data(train_input, train_labels), batch_size=batch_size, shuffle=shuffle)\n",
        "test_loader = DataLoader(CSV_Data(test_input, test_labels), batch_size=batch_size, shuffle=False)\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "\n",
        "print(len(train_loader)*batch_size)\n",
        "print(len(test_loader)*batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## random Baseline"
      ],
      "metadata": {
        "id": "5pzwXR87kCyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = net(seed=17) # depending on the initial seed, the model mighz classify all images as mel, achieving relatively high accuracy since the class is relatively large\n",
        "test_model(baseline, train_loader)\n",
        "print(\"---\")\n",
        "test_model(baseline, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_D_Ovo5jPFp",
        "outputId": "9723ce69-e584-4fd6-e96a-3cd15d84d876"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy for akiec: 163/261 | accuracy in %: 62.45210727969349\n",
            "accuracy for bcc: 6/414 | accuracy in %: 1.4492753623188406\n",
            "accuracy for bkl: 0/904 | accuracy in %: 0.0\n",
            "accuracy for df: 0/86 | accuracy in %: 0.0\n",
            "accuracy for mel: 0/5351 | accuracy in %: 0.0\n",
            "accuracy for nv: 0/115 | accuracy in %: 0.0\n",
            "accuracy for vasc: 399/884 | accuracy in %: 45.13574660633484\n",
            "overall accuracy: 568 / 8015 | accuracy in percent % 7.0867124142233315\n",
            "---\n",
            "accuracy for akiec: 44/66 | accuracy in %: 66.66666666666666\n",
            "accuracy for bcc: 1/100 | accuracy in %: 1.0\n",
            "accuracy for bkl: 0/195 | accuracy in %: 0.0\n",
            "accuracy for df: 0/29 | accuracy in %: 0.0\n",
            "accuracy for mel: 0/1354 | accuracy in %: 0.0\n",
            "accuracy for nv: 0/27 | accuracy in %: 0.0\n",
            "accuracy for vasc: 103/229 | accuracy in %: 44.97816593886463\n",
            "overall accuracy: 148 / 2000 | accuracy in percent % 7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFKc7523SJIC"
      },
      "source": [
        "## Model 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7DllPstESJIC",
        "outputId": "6b347355-518c-4f18-8835-6626e28ab768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "LwCL5yltSJIC"
      },
      "outputs": [],
      "source": [
        "class net(nn.Module):\n",
        "    def __init__(self, dropout = False, seed=17):\n",
        "        super(net, self).__init__()\n",
        "        torch.manual_seed(seed)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=0)\n",
        "        \n",
        "        self.fc1 = nn.Linear(32 * 6 * 6, 512)\n",
        "        self.fc2 = nn.Linear(512, 100)\n",
        "        self.fc3 = nn.Linear(100, 7)\n",
        "\n",
        "        # less droupout in deeper layers\n",
        "        self.dropout = False \n",
        "        self.dropout_3 = nn.Dropout(p=0.3) \n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "    def forward(self, x, train=True):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1, 32 * 6 * 6)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        if train and self.dropout:\n",
        "            x = self.dropout_5(x)    \n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        if train and self.dropout:\n",
        "            x = self.dropout_3(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "classes = ('akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "uZA9ZGzXSJID"
      },
      "outputs": [],
      "source": [
        "# train loop\n",
        "def train_model(model, data_loader, epochs, lr=0.1, optim=None):\n",
        "  if optim is None:\n",
        "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
        "  else:\n",
        "    optimizer = optim\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  \n",
        "  model.to(device)\n",
        "  for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      epoch_loss += loss.item()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f\"average batch loss in {epoch+1}: {epoch_loss/len(data_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "nFNo5RJKSJID"
      },
      "outputs": [],
      "source": [
        "# test loop\n",
        "def test_model(model, data_loader):\n",
        "  model.to(device)\n",
        "  with torch.no_grad():\n",
        "    right, wrong = np.array([0 for i in range(10)]), np.array([0 for i in range(10)])\n",
        "    for batch, (X,y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X, train=False)\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      for i in range(len(y_pred)):\n",
        "        if y_pred[i]==y[i]:\n",
        "          right[y[i]] += 1\n",
        "        else:\n",
        "         wrong[y[i]] += 1\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    print(f\"accuracy for {classes[i]}: {right[i]}/{right[i]+wrong[i]} | accuracy in %: {right[i]/(right[i]+wrong[i])*100}\")\n",
        "  \n",
        "  print(f\"overall accuracy: {np.sum(right)} / {np.sum(right) + np.sum(wrong)} | accuracy in percent % {100*np.sum(right)/(np.sum(right) + np.sum(wrong))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "2iDlPXpBSJID",
        "outputId": "41c82b30-7520-4270-80e3-0c5c2540fd96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:1\n",
            "average batch loss in 1: 1.049974169976221\n",
            "average batch loss in 2: 0.9300510172180073\n",
            "average batch loss in 3: 0.884845799433733\n",
            "average batch loss in 4: 0.8498187974511744\n",
            "average batch loss in 5: 0.8199817973101686\n",
            "average batch loss in 6: 0.7759172544032038\n",
            "average batch loss in 7: 0.7514441572204084\n",
            "average batch loss in 8: 0.7109242270211735\n",
            "average batch loss in 9: 0.6758476552551616\n",
            "average batch loss in 10: 0.6297376193626674\n",
            "iteration:2\n",
            "average batch loss in 1: 0.5019356618800801\n",
            "average batch loss in 2: 0.4454693213224173\n",
            "average batch loss in 3: 0.4076186245430016\n",
            "average batch loss in 4: 0.35960420145246086\n",
            "average batch loss in 5: 0.31475842865314074\n",
            "average batch loss in 6: 0.2827510247955065\n",
            "average batch loss in 7: 0.2474949287451402\n",
            "average batch loss in 8: 0.21039273906053005\n",
            "average batch loss in 9: 0.1909466605845415\n",
            "average batch loss in 10: 0.16382146018283988\n",
            "iteration:3\n",
            "average batch loss in 1: 0.07398186001077131\n",
            "average batch loss in 2: 0.04608437765952936\n",
            "average batch loss in 3: 0.031078798740315893\n",
            "average batch loss in 4: 0.01979493403745938\n",
            "average batch loss in 5: 0.012316378637355475\n",
            "average batch loss in 6: 0.009209222765302495\n",
            "average batch loss in 7: 0.005968666417843025\n",
            "average batch loss in 8: 0.0043474563993108105\n",
            "average batch loss in 9: 0.003576611270647642\n",
            "average batch loss in 10: 0.0030449150398709603\n",
            "iteration:4\n",
            "average batch loss in 1: 0.0025965371151141516\n",
            "average batch loss in 2: 0.0023437053893146497\n",
            "average batch loss in 3: 0.002168214863170628\n",
            "average batch loss in 4: 0.0020091206407485372\n",
            "average batch loss in 5: 0.001884031460950771\n",
            "average batch loss in 6: 0.0017628067096034455\n",
            "average batch loss in 7: 0.0016553413217784809\n",
            "average batch loss in 8: 0.0015542698957740763\n",
            "average batch loss in 9: 0.0014691172771693994\n",
            "average batch loss in 10: 0.0013980376546145318\n",
            "iteration:5\n",
            "average batch loss in 1: 0.0013187929376407468\n",
            "average batch loss in 2: 0.00126506747095439\n",
            "average batch loss in 3: 0.0012246344988611354\n",
            "average batch loss in 4: 0.001177022007919993\n",
            "average batch loss in 5: 0.0011359901549251312\n",
            "average batch loss in 6: 0.0010958888851528609\n",
            "average batch loss in 7: 0.0010630377357098448\n",
            "average batch loss in 8: 0.001030756118348063\n",
            "average batch loss in 9: 0.0009974571559701916\n",
            "average batch loss in 10: 0.0009700678175211655\n",
            "iteration:6\n",
            "average batch loss in 1: 0.000935547577690144\n",
            "average batch loss in 2: 0.0009127958635004605\n",
            "average batch loss in 3: 0.0008912387963578648\n",
            "average batch loss in 4: 0.0008704521859447245\n",
            "average batch loss in 5: 0.0008524940238569974\n",
            "average batch loss in 6: 0.0008335534915344117\n",
            "average batch loss in 7: 0.0008159293833951105\n",
            "average batch loss in 8: 0.0007971134400359441\n",
            "average batch loss in 9: 0.0007817118440124216\n",
            "average batch loss in 10: 0.0007665645862507082\n",
            "iteration:7\n",
            "average batch loss in 1: 0.0007487947978678242\n",
            "average batch loss in 2: 0.000735681487596681\n",
            "average batch loss in 3: 0.0007237398176530514\n",
            "average batch loss in 4: 0.0007104057333522006\n",
            "average batch loss in 5: 0.0007000467308279769\n",
            "average batch loss in 6: 0.0006885304766144767\n",
            "average batch loss in 7: 0.0006781021887230575\n",
            "average batch loss in 8: 0.0006673581121936013\n",
            "average batch loss in 9: 0.000655639542592852\n",
            "average batch loss in 10: 0.0006475353511396424\n",
            "iteration:8\n",
            "average batch loss in 1: 0.0006353658456688125\n",
            "average batch loss in 2: 0.0006277800972591235\n",
            "average batch loss in 3: 0.0006198774040030966\n",
            "average batch loss in 4: 0.0006121681852458417\n",
            "average batch loss in 5: 0.0006041123745847666\n",
            "average batch loss in 6: 0.0005968327549019663\n",
            "average batch loss in 7: 0.0005898118447013092\n",
            "average batch loss in 8: 0.0005824371936417114\n",
            "average batch loss in 9: 0.0005749772326280781\n",
            "average batch loss in 10: 0.0005680449884495826\n",
            "iteration:9\n",
            "average batch loss in 1: 0.0005611915235748168\n",
            "average batch loss in 2: 0.0005548574676913904\n",
            "average batch loss in 3: 0.0005495328033809316\n",
            "average batch loss in 4: 0.0005438909034336268\n",
            "average batch loss in 5: 0.0005378508922766417\n",
            "average batch loss in 6: 0.0005334116882048459\n",
            "average batch loss in 7: 0.000527635025955061\n",
            "average batch loss in 8: 0.0005224147867764572\n",
            "average batch loss in 9: 0.0005174414623605864\n",
            "average batch loss in 10: 0.0005123908709382469\n",
            "iteration:10\n",
            "average batch loss in 1: 0.0005070206392113771\n",
            "average batch loss in 2: 0.0005020450397090328\n",
            "average batch loss in 3: 0.0004982419225502064\n",
            "average batch loss in 4: 0.0004939526574772261\n",
            "average batch loss in 5: 0.0004902884377601705\n",
            "average batch loss in 6: 0.000486005817620679\n",
            "average batch loss in 7: 0.00048208197698252925\n",
            "average batch loss in 8: 0.000477933945624458\n",
            "average batch loss in 9: 0.00047377404048447514\n",
            "average batch loss in 10: 0.000470089033322024\n"
          ]
        }
      ],
      "source": [
        "model_0 = net()\n",
        "for i in range(10):\n",
        "  print(f\"iteration:{i+1}\")\n",
        "  train_model(model_0, train_loader, epochs=10, lr=0.01/(i+1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model_0, train_loader)\n",
        "print(\"---\")\n",
        "test_model(model_0, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1VyhXP5b15F",
        "outputId": "175ee676-d372-4323-cea8-1e6939a0acd9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy for akiec: 261/261 | accuracy in %: 100.0\n",
            "accuracy for bcc: 414/414 | accuracy in %: 100.0\n",
            "accuracy for bkl: 904/904 | accuracy in %: 100.0\n",
            "accuracy for df: 86/86 | accuracy in %: 100.0\n",
            "accuracy for mel: 5351/5351 | accuracy in %: 100.0\n",
            "accuracy for nv: 115/115 | accuracy in %: 100.0\n",
            "accuracy for vasc: 884/884 | accuracy in %: 100.0\n",
            "overall accuracy: 8015 / 8015 | accuracy in percent % 100.0\n",
            "---\n",
            "accuracy for akiec: 12/66 | accuracy in %: 18.181818181818183\n",
            "accuracy for bcc: 44/100 | accuracy in %: 44.0\n",
            "accuracy for bkl: 84/195 | accuracy in %: 43.07692307692308\n",
            "accuracy for df: 6/29 | accuracy in %: 20.689655172413794\n",
            "accuracy for mel: 1210/1354 | accuracy in %: 89.36484490398819\n",
            "accuracy for nv: 13/27 | accuracy in %: 48.148148148148145\n",
            "accuracy for vasc: 72/229 | accuracy in %: 31.4410480349345\n",
            "overall accuracy: 1441 / 2000 | accuracy in percent % 72.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1"
      ],
      "metadata": {
        "id": "a_TxfvGJdwnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problems with model 0:\n",
        "\n",
        "\n",
        "1.   Overfitting\n",
        "2.   Number of samples is not evenly distributed, which leads to the model being biased in favour of more frequent classes.\n",
        "\n",
        "Mittigation:\n",
        "1. data augmentation\n",
        "2. regularization\n",
        "3. batch normalization\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2t9tiisGc7Jb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4ynhlD0e1Ym",
        "outputId": "c0395bab-7b55-45d5-b8d9-d9a9e5b4859b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2004\n",
            "500\n",
            "32064\n",
            "8000\n"
          ]
        }
      ],
      "source": [
        "# data augmentation\n",
        "# first we just rotate all images and add them to the train/test set, later we might only pictures such that the classes are all about the same size\n",
        "\n",
        "batch_size = 16\n",
        "shuffle = True\n",
        "\n",
        "aug_train_inputs = []\n",
        "aug_train_labels = []\n",
        "aug_test_inputs = []\n",
        "aug_test_labels = []\n",
        "\n",
        "for index, image in enumerate(data):\n",
        "    image = [float(pixel) for pixel in image]\n",
        "    tensor_image = torch.tensor([[image[:-1][0:28*28][i*28:(i+1)*28] for i in range(28)], [image[:-1][28*28:2*28*28][i*28:(i+1)*28] for i in range(28)], [image[:-1][2*28*28:][i*28:(i+1)*28] for i in range(28)]], dtype=torch.float32)\n",
        "    if index in test_split:\n",
        "      aug_test_inputs.append(tensor_image)\n",
        "      aug_test_labels.append(torch.tensor(image[-1], dtype=torch.long))\n",
        "      for i in [1,2,3]:\n",
        "        aug_test_inputs.append(torch.rot90(tensor_image, i, [1, 2]))\n",
        "        aug_test_labels.append(torch.tensor(image[-1], dtype=torch.long))\n",
        "    else:\n",
        "      aug_train_inputs.append(tensor_image)\n",
        "      aug_train_labels.append(torch.tensor(image[-1], dtype=torch.long))\n",
        "      for i in [1,2,3]:\n",
        "        aug_train_inputs.append(torch.rot90(tensor_image, i, [1, 2]))\n",
        "        aug_train_labels.append(torch.tensor(image[-1], dtype=torch.long))\n",
        "aug_train_loader = DataLoader(CSV_Data(aug_train_inputs, aug_train_labels), batch_size=batch_size, shuffle=shuffle)\n",
        "aug_test_loader = DataLoader(CSV_Data(aug_test_inputs, aug_test_labels), batch_size=batch_size, shuffle=False)\n",
        "print(len(aug_train_loader))\n",
        "print(len(aug_test_loader))\n",
        "print(len(aug_train_loader)*batch_size)\n",
        "print(len(aug_test_loader)*batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug_model_0 = net()\n",
        "for i in range(0,10):\n",
        "  print(f\"iteration:{i+1}\")\n",
        "  train_model(aug_model_0, aug_train_loader, epochs=10, lr=0.01/(i+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR8sm718hjwg",
        "outputId": "829b9306-1126-4f12-f29a-15f5d495538d"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:1\n",
            "average batch loss in 1: 1.0428919268998795\n",
            "average batch loss in 2: 0.9222879297139045\n",
            "average batch loss in 3: 0.8845254626281247\n",
            "average batch loss in 4: 0.849607192314492\n",
            "average batch loss in 5: 0.8226646314823104\n",
            "average batch loss in 6: 0.7968466394839292\n",
            "average batch loss in 7: 0.7703361720471325\n",
            "average batch loss in 8: 0.7445546372936752\n",
            "average batch loss in 9: 0.7244395885891662\n",
            "average batch loss in 10: 0.699477148084286\n",
            "iteration:2\n",
            "average batch loss in 1: 0.5850905268239285\n",
            "average batch loss in 2: 0.5421568215592297\n",
            "average batch loss in 3: 0.5040901447190883\n",
            "average batch loss in 4: 0.4765532100777664\n",
            "average batch loss in 5: 0.4473450482021757\n",
            "average batch loss in 6: 0.42238394958812914\n",
            "average batch loss in 7: 0.40113424159541816\n",
            "average batch loss in 8: 0.37279567419237153\n",
            "average batch loss in 9: 0.34962549436504686\n",
            "average batch loss in 10: 0.34024955724791706\n",
            "iteration:3\n",
            "average batch loss in 1: 0.22414609870525773\n",
            "average batch loss in 2: 0.18353549020837512\n",
            "average batch loss in 3: 0.15960847578690426\n",
            "average batch loss in 4: 0.14967510195371383\n",
            "average batch loss in 5: 0.1419262591105602\n",
            "average batch loss in 6: 0.14010676151288962\n",
            "average batch loss in 7: 0.14659799852867042\n",
            "average batch loss in 8: 0.13421271593935177\n",
            "average batch loss in 9: 0.12326252705123282\n",
            "average batch loss in 10: 0.1364398014475028\n",
            "iteration:4\n",
            "average batch loss in 1: 0.07638920897632111\n",
            "average batch loss in 2: 0.04294480208858851\n",
            "average batch loss in 3: 0.026130349978183642\n",
            "average batch loss in 4: 0.029106988070423143\n",
            "average batch loss in 5: 0.020837712844896994\n",
            "average batch loss in 6: 0.014881470999937224\n",
            "average batch loss in 7: 0.009987834710641609\n",
            "average batch loss in 8: 0.006248584539649553\n",
            "average batch loss in 9: 0.003533766885104075\n",
            "average batch loss in 10: 0.0021202301066342803\n",
            "iteration:5\n",
            "average batch loss in 1: 0.00157690564027051\n",
            "average batch loss in 2: 0.0012889607971384046\n",
            "average batch loss in 3: 0.0011195304642350873\n",
            "average batch loss in 4: 0.0009979980669551091\n",
            "average batch loss in 5: 0.0009042654406235913\n",
            "average batch loss in 6: 0.0008301600934355102\n",
            "average batch loss in 7: 0.0007637265139974776\n",
            "average batch loss in 8: 0.0007095947766811411\n",
            "average batch loss in 9: 0.0006642530695537898\n",
            "average batch loss in 10: 0.0006224475967589786\n",
            "iteration:6\n",
            "average batch loss in 1: 0.0005806399990068646\n",
            "average batch loss in 2: 0.0005529887168802849\n",
            "average batch loss in 3: 0.0005306360298458323\n",
            "average batch loss in 4: 0.0005078565235658555\n",
            "average batch loss in 5: 0.0004872013107691221\n",
            "average batch loss in 6: 0.00046834422269487696\n",
            "average batch loss in 7: 0.00045081321135896163\n",
            "average batch loss in 8: 0.0004337441508816667\n",
            "average batch loss in 9: 0.0004185319848500035\n",
            "average batch loss in 10: 0.0004042289094484717\n",
            "iteration:7\n",
            "average batch loss in 1: 0.00038993634123137087\n",
            "average batch loss in 2: 0.00037900651500285946\n",
            "average batch loss in 3: 0.0003688961139802186\n",
            "average batch loss in 4: 0.00035937183054930466\n",
            "average batch loss in 5: 0.00034970600861674244\n",
            "average batch loss in 6: 0.00034126082083603334\n",
            "average batch loss in 7: 0.00033288367706775187\n",
            "average batch loss in 8: 0.00032514514573808945\n",
            "average batch loss in 9: 0.0003175901202793826\n",
            "average batch loss in 10: 0.0003104083600521614\n",
            "iteration:8\n",
            "average batch loss in 1: 0.00030270403045810625\n",
            "average batch loss in 2: 0.0002970064623372842\n",
            "average batch loss in 3: 0.0002910430998178198\n",
            "average batch loss in 4: 0.0002861506418313183\n",
            "average batch loss in 5: 0.0002808255231847725\n",
            "average batch loss in 6: 0.0002757001870943618\n",
            "average batch loss in 7: 0.00027093917032749504\n",
            "average batch loss in 8: 0.0002661462213692081\n",
            "average batch loss in 9: 0.0002617113050094811\n",
            "average batch loss in 10: 0.00025716737949023555\n",
            "iteration:9\n",
            "average batch loss in 1: 0.00025295326803638274\n",
            "average batch loss in 2: 0.0002492037030070965\n",
            "average batch loss in 3: 0.000245612400631376\n",
            "average batch loss in 4: 0.00024211778954171955\n",
            "average batch loss in 5: 0.00023890694465672436\n",
            "average batch loss in 6: 0.00023550185423108154\n",
            "average batch loss in 7: 0.00023236757437450063\n",
            "average batch loss in 8: 0.00022926838688274877\n",
            "average batch loss in 9: 0.0002261845615217294\n",
            "average batch loss in 10: 0.00022321196912827957\n",
            "iteration:10\n",
            "average batch loss in 1: 0.00022022384941801883\n",
            "average batch loss in 2: 0.0002175962099983507\n",
            "average batch loss in 3: 0.00021520396426198343\n",
            "average batch loss in 4: 0.00021275610656500146\n",
            "average batch loss in 5: 0.00021038967862332985\n",
            "average batch loss in 6: 0.0002081138654835212\n",
            "average batch loss in 7: 0.0002059298810910226\n",
            "average batch loss in 8: 0.0002036506631236971\n",
            "average batch loss in 9: 0.00020145171077109853\n",
            "average batch loss in 10: 0.00019937903716758614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(aug_model_0, aug_train_loader)\n",
        "print(\"---\")\n",
        "test_model(aug_model_0, aug_test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYibTccWlDCh",
        "outputId": "ce99a3d3-9ce7-48ae-d6b3-d9cb2f56c6e0"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy for akiec: 1044/1044 | accuracy in %: 100.0\n",
            "accuracy for bcc: 1656/1656 | accuracy in %: 100.0\n",
            "accuracy for bkl: 3616/3616 | accuracy in %: 100.0\n",
            "accuracy for df: 344/344 | accuracy in %: 100.0\n",
            "accuracy for mel: 21404/21404 | accuracy in %: 100.0\n",
            "accuracy for nv: 460/460 | accuracy in %: 100.0\n",
            "accuracy for vasc: 3536/3536 | accuracy in %: 100.0\n",
            "overall accuracy: 32060 / 32060 | accuracy in percent % 100.0\n",
            "---\n",
            "accuracy for akiec: 57/264 | accuracy in %: 21.59090909090909\n",
            "accuracy for bcc: 146/400 | accuracy in %: 36.5\n",
            "accuracy for bkl: 253/780 | accuracy in %: 32.43589743589744\n",
            "accuracy for df: 24/116 | accuracy in %: 20.689655172413794\n",
            "accuracy for mel: 4777/5416 | accuracy in %: 88.2016248153619\n",
            "accuracy for nv: 49/108 | accuracy in %: 45.370370370370374\n",
            "accuracy for vasc: 266/916 | accuracy in %: 29.03930131004367\n",
            "overall accuracy: 5572 / 8000 | accuracy in percent % 69.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug_dropout_model_0 = net(dropout=True)\n",
        "for i in range(0,10,2):\n",
        "  print(f\"iteration:{i+1}\")\n",
        "  train_model(aug_dropout_model_0, aug_train_loader, epochs=10, lr=0.01/(i+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBqG2DX-iwdJ",
        "outputId": "12e36d3d-8998-4554-c9c4-c2763cfed6da"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:1\n",
            "average batch loss in 1: 1.0346965776187573\n",
            "average batch loss in 2: 0.904344401606721\n",
            "average batch loss in 3: 0.8592819171453903\n",
            "average batch loss in 4: 0.8205597791263086\n",
            "average batch loss in 5: 0.7864577781981574\n",
            "average batch loss in 6: 0.7534084853729207\n",
            "average batch loss in 7: 0.7192131275322265\n",
            "average batch loss in 8: 0.6834684544635152\n",
            "average batch loss in 9: 0.6527785609426078\n",
            "average batch loss in 10: 0.6146692164993274\n",
            "iteration:3\n",
            "average batch loss in 1: 0.44561731751391037\n",
            "average batch loss in 2: 0.38072895543452745\n",
            "average batch loss in 3: 0.3339132139805013\n",
            "average batch loss in 4: 0.2936805110297421\n",
            "average batch loss in 5: 0.25460054973575347\n",
            "average batch loss in 6: 0.2271442684398856\n",
            "average batch loss in 7: 0.19853924664049716\n",
            "average batch loss in 8: 0.18346364349240676\n",
            "average batch loss in 9: 0.15456232328598524\n",
            "average batch loss in 10: 0.14655943787974485\n",
            "iteration:5\n",
            "average batch loss in 1: 0.0756050420117782\n",
            "average batch loss in 2: 0.043822950529068906\n",
            "average batch loss in 3: 0.03120771764436662\n",
            "average batch loss in 4: 0.021176611467930553\n",
            "average batch loss in 5: 0.015079212821608838\n",
            "average batch loss in 6: 0.010991824155517933\n",
            "average batch loss in 7: 0.00691499099334914\n",
            "average batch loss in 8: 0.004874396090471493\n",
            "average batch loss in 9: 0.003791726094300384\n",
            "average batch loss in 10: 0.0030263244432194507\n",
            "iteration:7\n",
            "average batch loss in 1: 0.002402254207223966\n",
            "average batch loss in 2: 0.002137667197121935\n",
            "average batch loss in 3: 0.0019405814595427704\n",
            "average batch loss in 4: 0.001785404424189566\n",
            "average batch loss in 5: 0.0016516435615227046\n",
            "average batch loss in 6: 0.0015418221918096465\n",
            "average batch loss in 7: 0.0014354457576302028\n",
            "average batch loss in 8: 0.0013518983678899232\n",
            "average batch loss in 9: 0.0012684959829686046\n",
            "average batch loss in 10: 0.0012003627555338722\n",
            "iteration:9\n",
            "average batch loss in 1: 0.001123072542671247\n",
            "average batch loss in 2: 0.001079376501843013\n",
            "average batch loss in 3: 0.0010363016730520404\n",
            "average batch loss in 4: 0.0009998864584685326\n",
            "average batch loss in 5: 0.000964425557269184\n",
            "average batch loss in 6: 0.0009297805102853078\n",
            "average batch loss in 7: 0.0008996864466018611\n",
            "average batch loss in 8: 0.0008705019904762715\n",
            "average batch loss in 9: 0.0008425493722202527\n",
            "average batch loss in 10: 0.0008180603452218858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(aug_dropout_model_0, aug_train_loader)\n",
        "print(\"---\")\n",
        "test_model(aug_dropout_model_0, aug_test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a636798-0bfa-49d2-e8da-dc3c111760b0",
        "id": "KtX05pIin2Ao"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy for akiec: 1044/1044 | accuracy in %: 100.0\n",
            "accuracy for bcc: 1656/1656 | accuracy in %: 100.0\n",
            "accuracy for bkl: 3616/3616 | accuracy in %: 100.0\n",
            "accuracy for df: 344/344 | accuracy in %: 100.0\n",
            "accuracy for mel: 21404/21404 | accuracy in %: 100.0\n",
            "accuracy for nv: 460/460 | accuracy in %: 100.0\n",
            "accuracy for vasc: 3536/3536 | accuracy in %: 100.0\n",
            "overall accuracy: 32060 / 32060 | accuracy in percent % 100.0\n",
            "---\n",
            "accuracy for akiec: 62/264 | accuracy in %: 23.484848484848484\n",
            "accuracy for bcc: 144/400 | accuracy in %: 36.0\n",
            "accuracy for bkl: 292/780 | accuracy in %: 37.43589743589744\n",
            "accuracy for df: 17/116 | accuracy in %: 14.655172413793101\n",
            "accuracy for mel: 4806/5416 | accuracy in %: 88.7370753323486\n",
            "accuracy for nv: 52/108 | accuracy in %: 48.148148148148145\n",
            "accuracy for vasc: 266/916 | accuracy in %: 29.03930131004367\n",
            "overall accuracy: 5639 / 8000 | accuracy in percent % 70.4875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}